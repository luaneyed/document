## Chapter 22 연속적 장애 다루기

`처음에 성공하지 못하면 그 이상으로 늦어지게 된다.`
`왜 사람들은 조금만 더 신경을 쓰면 된다는 사실을 늘 잊곤 하는가?`

### 연속적 장애의 원인과 그 대책

#### 서버 과부하
일부 클러스터의 오류가 다른 클러스터의 부하를 증가시키며, 나머지 클러스터들도 과부하가 발생하며 차례대로 죽어가는 상황

#### 자원의 부족

##### CPU

모든 요청의 처리가 느려지므로 다음과 같은 부차적인 현상이 발생

- 처리 중인 요청 수의 증가로 메모리, thread 수, file descriptor 등에 영향
- 서버 큐가 엄청 커며 메모리를 많이 소비
- 스레드 기아 (thread starvation)
- CPU 혹은 요청 기아 (원격 와치독 이벤트를 요청 큐로 처리하는 경우)
- 기껏 힘들게 RPC 요청을 처리했더니 client에서 timeout하여 재요청
- CPU 캐시 이점 감소

##### 메모리

메모리 기아(memory starvation)가 유발하는 현상

- 태스크 종료
- GC 작업이 빈번해지면서 CPU 사용률 증가
- RAM이 부족하므로 캐시 활용률 감소

##### 스레드

- 너무 많은 RAM 소비
- health check 실패
- (극단적인 경우) process id 소진

##### 파일 서술자 (File descriptor)

- 최대 갯수를 모두 사용하면 네트워크 연결이 안 되므로 health check 실패

##### 자원 간의 의존성

- 부차적 증상은 마치 근본 원인같아서, 디버깅이 어려움
- 의존 관계의 여러 자원의 담당자가 다르다면 더 어려움
- ex) 백엔드와 프론트엔드가 메모리를 공유하는 상황에서 health check 실패
- ex) 사실은 프론트엔드 GC parameter 설정 오류가 근본 원인

#### 서비스 이용 불가

자원이 부족하면 결국 일부 요청은 처리할 수 없음. 로드밸런싱 수용량이 감소하며 레임덕 상태가 되거나 health check 실패.

### 서버 과부하 방지

아래는 과부하 방지를 위한 전략들을 나열

##### 서버 부하 테스트
##### 과부하 상황에서의 실패 테스트
##### 경감된 응답 제공
쉬운 연산, 낮은 품질
##### 과부하 상태에서 요청 거부
##### 고수준의 시스템들이 서버에 과부하를 유발하지 않고 요청을 거부하도록 구현
- 리버스 프록시로 IP 주소 등을 이용하여 요청 제한
- 로드밸런서로 특정 요청 차단
##### 수용량 계획 실행
- 연속적 장애의 가능성을 줄일 수 있음
- 반드시 성능 테스트와 병행
- 서비스 전체의 최대 부하, 각 클러스터의 QPS 한계, N + 2 공식

#### 큐 관리하기
- 트래픽이 비슷한 시스템 : 스레드 풀 크기의 50% 이하로 큐 길이 설정. 많은 요청을 최대한 일찍 거부
- 트래픽 변화폭이 큰 시스템 : 과부하 상황을 고려하여 큐 크기 결정

#### 부하 제한과 적절한 퇴보
- 일정 수 이상의 요청에 `HTTP 503` 리턴
- 후입선출(LIFO)을 사용
- 요청에 우선 순위(priority) 설정

##### 적절한 퇴보(graceful degradation)
- ex) db나 아닌 memory caching에서만 검색 수행
- ex) 정확도가 떨어지지만 더 빠른 순위 알고리즘 적용

##### 부하 제한이나 적절한 퇴보를 적용할 때 고려 사항
- 어떤 지표를 활용하여 적용할 것인가?
- 퇴보 모드에 들어갔다면 어떤 조치를 취해야 할까?
- 어느 계층에서 구현해야 할까?
- 너무 자주 적절한 퇴보가 발생하면 안 된다.
- 일부러라도 정기적으로 적절한 퇴보 모드 운영
- 적절한 퇴보 모드로 동작하는 서버들 모니터링
- 적절한 퇴보 로직을 신속하게 on/off 할 수 있도록

#### 재시도
과부하 상황에서의 반복적인 재시도는 요청 실패 비율을 점점 높인다.

##### 자동 재시도를 수행할 때 고려할 사항들
- 임의의 값을 이용해 지수적으로 간격을 두기
- 재시도 최댓값 설정
- 서버 수준에서 프로세스 별로 재시도 허용 최댓값 설정
- 서비스 전체를 보고 특정 수준의 재시도가 정말로 필요한지 결정
- 명확한 응답 코드를 사용. 과부하 시에는 특정 상태를 리턴해서 재시도하지 않도록.

#### 지연응답과 마감기한

##### 마감기한의 상실
서버가 복잡한 요청을 처리하기에 앞서 timeout을 확인해야 한다

##### 마감기한 전파
- 네트워크 및 클라이언트 처리 등을 고려하여 조금 더 들려서 전달해도 무방
- 외부 시스템에 전달할 마감기한의 상한값을 설정해놓는 것도 고려
- 마감기한이 지나서 처리하는 예외 상황도 있음
- 작업취소 전파 vs 마감기한 전파 (318p)

##### 이중 지연응답
일부 지연응답이 나머지 요청도 에러를 리턴하게 만드는 상황

- 사전 인지가 매우 어려움. 일시적인 지연 응답이 이중 지연응답인지 판단도 어려움.
- 마감기한까지 기다리지 말고 일찍 에러 리턴
- 몇 배나 긴 마감기한을 설정하는 것은 좋지 않음.
- 한 클라이언트가 최대 25%의 스레드만 점유할 수 있도록 설정

### 느긋한 시작과 콜드 캐싱

프로세스 시작 직후의 속도 저하

##### 첫 요청을 받을 때 백엔드와 연결
##### 런타임 성능 향상을 위한 추가 작업 (JIT 컴파일 등)
##### 콜드 캐시 상태
- 새로 추가한 클러스터
- 유지보수 후 (기존 캐시 유용성 X)
- 서비스 재시작 (레디스, 멤캐시로 보완 가능)

##### 캐시가 중요할 때의 전략
- 서비스 오버프로비전. 지연응답 캐시와 용량 캐시를 구분.
- 일반적인 연속적 장애 방지 기법 적용
- 새 클러스터에 위임하는 부하의 크기는 천천히 늘려야 함. 캐시가 채워질 여유를 위해.

#### 항상 스택의 아래쪽을 살펴보자

백엔드끼리의 상호 통신을 점검

- **분산 데드락(distributed deadlock)** 가능
- 과부하 서버가 다른 서버에 일을 맡기는 형태라면, 많이ㅠ맡길수록 파싱과 대기 비용 증가
- 시스템의 부트스트랩이 복잡해짐. 차라리 프론트엔드가 직접 여러 백엔드와 통신하는 것이 나음.

### 연속적 장애의 발생 요인

#### 프로세스 중단
#### 프로세스 업데이트
새 바이너리나 새 설정이 많은 태스크에 영향을 주는 경우
#### 새로운 배포
연속적 장애가 발생하면 최근 변경을 롤백해보면 좋음
#### 유기적 성장
서비스 성장에 맞춰서 수용량 조정 필요
#### 계획에 의한 변경, 자원의 감소 혹은 서버의 종료

##### 요청 프로파일의 변화
개별 페이로드 처리 비용 변경 등

##### 자원의 제한
여분의 CPU를 믿지 말고, 제한된 자원으로 부하 테스트를 하라

### 연속적 장애 테스트하기

#### 장애가 발생할 때까지 테스트하고 조치하기
과부하 상태 테스트

- 이상적인 컴포넌트는 처리하는 요청의 양이 크게 줄지 않으면서도 부하의 한계를 넘어서 요청에 대해서는 에러 혹은 경감된 결과를 리턴해야 한다.
- 어느 지점에서 장애가 발생하는지 확인
- 부하의 갑작스러운 증가와 점진적 증가를 모두 테스트 (캐시 효과)
- 과부하 상태였다가 돌아온 후의 동작도 테스트하고 이해
	1. 퇴보 모드에 들어갔다면 사람의 개입 없이 알아서 잘 돌아오는가?
	2. 몇 개의 서버에서 충돌이 발생할 때 안정화시키려면 어느 정도나 부하가 절감되어야 하는가?
- 컴포넌트마다 별도로 부하 테스트 진행

##### 프로덕션 환경에서 직접 테스트
시스템이 안전하다고 믿는다면 프로덕션 환경의 일부를 대상으로 테스트해볼 수 있음

- 프로덕션 테스트 특징
	1. 부하 테스트보다 실제에 근접한 결과
	2. 하지만 사용자가 인지할 수도 있음
	3. 만일을 대비해 충분한 여분의 수용량 확보
- 테스트 예시
	1. 태스크의 수를 빠르게 혹은 천천히 줄여보기
	2. 클러스터 수용량을 신속하게 줄여보기
	3. 다양한 백엔드를 시스템에서 감춰보기

#### 사용량이 높은 클라이언트 테스트하기
- 서비스가 다운된 동안 얼마나 빠르게 작업을 처리하는가
- **임의의 지수 백오프(exponential backoff)**를 사용하는가
- 대량의 부하를 유발하는 취약점을 가지고 있는가 (외부에서 해당 클라이언트의 캐시를 삭제할 수 있다거나)

#### 상대적으로 덜 중요한 백엔드의 테스트
- 덜 중요한 백엔드가 지금 사용 불가능하더라도 테스트
- 프론트엔드가 덜 중요한 백엔드를 기다리면서 심각하게 느려질 수도 있음

### 연속적 장애를 처리하기 위한 즉각적인 대처

#### 자원의 추가 투입
시스템의 가용성이 줄어들었는데 여분의 자원이 있는 경우
단, **죽음의 소용돌이**에 휘말린 경우는 예외

#### Health Check 중지
Health Check 실패는 프로세스를 종료시키므로, 곧 건강해질 시스템마저 망해버릴 가능성 있음

#### 서버의 재시작
- **죽음의 GC 소용돌이**가 발생한 경우
- timeout이 없는 요청들 때문에 스레드 차단이 발생한 경우
- 서버에서 데드락이 발생한 경우

#### 트래픽의 경감
- 서버들이 건강한데도 충돌이 발생하면
	1. 기본 원인 처리 (수용량의 추가 투입 등)
	2. 충돌이 사라질 만큼 적극적으로 충분히 부하를 경감 (1%만 처리해도 될 정도로)
	3. 이렇게 하면 아마 괜찮아질거다
	4. 점진적으로 서버에 부하를 늘려간다

- 물론 이 방법을 사용하면 무조건 사용자는 장애를 경험한다.
- 어차피 다시 트래픽을 다 받기 시작하면 장애가 날 수 있으므로, 문제의 원인을 수정해야 한다.

#### 퇴보 모드로 들어가기

#### 일괄 작업 부하(batch load) 배제하기
인덱스 수정, 데이터 복사, 통계 등

#### 문제가 있는 트래픽 배제하기
너무 많은 부하나 충돌을 유발하는 쿼리를 차단하거나 배제
